{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkSA09sq_-o7",
        "outputId": "9e2f2aed-4993-4eb1-ed48-3b442569dc98"
      },
      "outputs": [],
      "source": [
        "!pip install ir-datasets\n",
        "import ir_datasets\n",
        "\n",
        "dataset = ir_datasets.load('cranfield')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_s06hg7AUT_",
        "outputId": "a41883fa-6a18-4ef1-9166-dcef253e5cc0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in tokens\n",
        "        if word not in stop_words and word.isalpha()\n",
        "    ]\n",
        "    return tokens\n",
        "\n",
        "processed_docs = [' '.join(preprocess_text(doc.text)) for doc in dataset.docs_iter()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFpDA8q-AcRt"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
        "feature_names = vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5tlZnzDAj3w",
        "outputId": "479cfb0b-46ad-4dfe-b3e5-6059ce528bd6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "processed_queries = []\n",
        "query_ids = []\n",
        "for query in dataset.queries_iter():\n",
        "    processed_queries.append(' '.join(preprocess_text(query.text)))\n",
        "    query_ids.append(query.query_id)\n",
        "\n",
        "tfidf_queries = vectorizer.transform(processed_queries)\n",
        "\n",
        "qrels_dict = {}\n",
        "for qrel in dataset.qrels_iter():\n",
        "    if qrel.relevance > 0:\n",
        "        qrels_dict.setdefault(qrel.query_id, set()).add(qrel.doc_id)\n",
        "\n",
        "doc_list = list(dataset.docs_iter())\n",
        "index_to_doc_id = [doc.doc_id for doc in doc_list]\n",
        "\n",
        "N = 10\n",
        "query_index_to_evaluate = 0\n",
        "query_id = query_ids[query_index_to_evaluate]\n",
        "\n",
        "query_doc_similarity = cosine_similarity(tfidf_queries[query_index_to_evaluate], tfidf_matrix)\n",
        "sorted_doc_indices = query_doc_similarity.flatten().argsort()[::-1]\n",
        "\n",
        "top_n_indices = sorted_doc_indices[:N]\n",
        "retrieved_doc_ids = [index_to_doc_id[i] for i in top_n_indices]\n",
        "\n",
        "relevant_doc_ids = qrels_dict.get(query_id, set())\n",
        "\n",
        "hits = sum(1 for doc_id in retrieved_doc_ids if doc_id in relevant_doc_ids)\n",
        "precision_at_N = hits / N if N > 0 else 0\n",
        "\n",
        "print(f\"\\nAvaliação para o ID da consulta: {query_id}\")\n",
        "print(f\"Número de documentos relevantes para esta consulta: {len(relevant_doc_ids)}\")\n",
        "print(f\"Top {N} IDs de documentos recuperados: {retrieved_doc_ids}\")\n",
        "print(f\"Número de documentos relevantes nos top {N}: {hits}\")\n",
        "print(f\"Precisão@{N}: {precision_at_N:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4RyK16JgIeeV",
        "outputId": "b5bad3c6-90e8-426d-ce68-e589ba81d14a"
      },
      "outputs": [],
      "source": [
        "def calculate_interpolated_metrics(ranked_doc_indices, relevant_doc_ids, doc_index_to_id, recall_levels):\n",
        "    precision_recall_points = []\n",
        "    hits = 0\n",
        "    num_relevant = len(relevant_doc_ids)\n",
        "    num_retrieved = 0\n",
        "\n",
        "    if num_relevant == 0:\n",
        "        return ([0] * len(recall_levels), [0] * len(recall_levels), [0] * len(recall_levels))\n",
        "\n",
        "    for doc_idx in ranked_doc_indices:\n",
        "        num_retrieved += 1\n",
        "        doc_id = doc_index_to_id[doc_idx]\n",
        "        if doc_id in relevant_doc_ids:\n",
        "            hits += 1\n",
        "\n",
        "        current_precision = hits / num_retrieved\n",
        "        current_recall = hits / num_relevant\n",
        "        precision_recall_points.append((current_precision, current_recall))\n",
        "\n",
        "    precision_recall_points.insert(0, (1.0, 0.0))\n",
        "    if precision_recall_points[-1][1] < 1:\n",
        "        precision_recall_points.append((0.0, 1.0))\n",
        "\n",
        "    precision_recall_points.sort(key=lambda x: x[1])\n",
        "    interpolated_precision = []\n",
        "    current_max_precision = 0\n",
        "\n",
        "    for i in range(len(precision_recall_points) - 1, -1, -1):\n",
        "        current_max_precision = max(current_max_precision, precision_recall_points[i][0])\n",
        "        interpolated_precision.insert(0, (current_max_precision, precision_recall_points[i][1]))\n",
        "\n",
        "    interpolated_precisions_at_levels = []\n",
        "    interpolated_recalls_at_levels = []\n",
        "    interpolated_f1_at_levels = []\n",
        "\n",
        "    for recall_level in recall_levels:\n",
        "        found_precision = 0.0\n",
        "        for prec, rec in interpolated_precision:\n",
        "            if rec >= recall_level:\n",
        "                found_precision = prec\n",
        "                break\n",
        "        interpolated_precisions_at_levels.append(found_precision)\n",
        "        interpolated_recalls_at_levels.append(recall_level)\n",
        "        if found_precision + recall_level > 0:\n",
        "            f1 = 2 * found_precision * recall_level / (found_precision + recall_level)\n",
        "        else:\n",
        "            f1 = 0.0\n",
        "        interpolated_f1_at_levels.append(f1)\n",
        "\n",
        "    return interpolated_precisions_at_levels, interpolated_recalls_at_levels, interpolated_f1_at_levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_interpolated_precision(ranked_doc_indices, relevant_doc_ids, doc_index_to_id, recall_levels):\n",
        "    \"\"\"Calculates interpolated precision at given recall levels for a single query.\"\"\"\n",
        "    precision_recall_points = []\n",
        "    hits = 0\n",
        "    num_relevant = len(relevant_doc_ids)\n",
        "    num_retrieved = 0\n",
        "\n",
        "    if num_relevant == 0:\n",
        "        return [0] * len(recall_levels)\n",
        "\n",
        "    for doc_idx in ranked_doc_indices:\n",
        "        num_retrieved += 1\n",
        "        doc_id = doc_index_to_id[doc_idx]\n",
        "        if doc_id in relevant_doc_ids:\n",
        "            hits += 1\n",
        "\n",
        "        current_precision = hits / num_retrieved\n",
        "        current_recall = hits / num_relevant\n",
        "        precision_recall_points.append((current_precision, current_recall))\n",
        "\n",
        "    precision_recall_points.insert(0, (0, 0))\n",
        "    if precision_recall_points[-1][1] < 1:\n",
        "         precision_recall_points.append((precision_recall_points[-1][0], 1.0))\n",
        "\n",
        "    precision_recall_points.sort(key=lambda x: x[1])\n",
        "    interpolated_precision = []\n",
        "    current_max_precision = 0\n",
        "\n",
        "    for i in range(len(precision_recall_points) - 1, -1, -1):\n",
        "         current_max_precision = max(current_max_precision, precision_recall_points[i][0])\n",
        "         interpolated_precision.insert(0, (current_max_precision, precision_recall_points[i][1]))\n",
        "\n",
        "    interpolated_precisions_at_levels = []\n",
        "    current_interpolated_idx = 0\n",
        "\n",
        "    for recall_level in recall_levels:\n",
        "        found_precision = 0.0\n",
        "        for prec, rec in interpolated_precision:\n",
        "             if rec >= recall_level:\n",
        "                  found_precision = prec\n",
        "                  break\n",
        "        interpolated_precisions_at_levels.append(found_precision)\n",
        "    return interpolated_precisions_at_levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_interpolated_precisions_per_query = []\n",
        "num_queries_for_11pt_eval_tfidf = 0\n",
        "\n",
        "for query_idx, query_id in enumerate(query_ids):\n",
        "    current_relevant_doc_ids = qrels_dict.get(query_id, set())\n",
        "    if not current_relevant_doc_ids:\n",
        "        continue\n",
        "\n",
        "    num_queries_for_11pt_eval_tfidf += 1\n",
        "    query_similarity = cosine_similarity(tfidf_queries[query_idx], tfidf_matrix)\n",
        "    tfidf_ranked_doc_indices = query_similarity.flatten().argsort()[::-1]\n",
        "    interpolated_precisions = calculate_interpolated_precision(\n",
        "        tfidf_ranked_doc_indices, current_relevant_doc_ids, index_to_doc_id, recall_levels_11pt\n",
        "    )\n",
        "    tfidf_interpolated_precisions_per_query.append(interpolated_precisions)\n",
        "\n",
        "if num_queries_for_11pt_eval_tfidf > 0:\n",
        "    avg_tfidf_interpolated_precision = np.mean(tfidf_interpolated_precisions_per_query, axis=0)\n",
        "else:\n",
        "    avg_tfidf_interpolated_precision = [0] * len(recall_levels_11pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_interpolated_f1(interpolated_precisions, rec):\n",
        "    interpolated_f1 = []\n",
        "    for p, r in zip(interpolated_precisions, rec):\n",
        "        if p + r == 0:\n",
        "            interpolated_f1.append(0.0)\n",
        "        else:\n",
        "            interpolated_f1.append(2 * p * r / (p + r))\n",
        "    return interpolated_f1\n",
        "\n",
        "avg_tfidf_f1 = calculate_interpolated_f1(avg_tfidf_interpolated_precision, avg_tfidf_rec)\n",
        "avg_tfidf_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "avg_metrics_df = pd.DataFrame({\n",
        "    'interpolated_recalls_at_levels': avg_tfidf_rec,\n",
        "    'interpolated_precisions_at_levels': avg_tfidf_interpolated_precision,\n",
        "    'interpolated_f1_at_levels': avg_tfidf_f1,\n",
        "})\n",
        "\n",
        "avg_metrics_df.to_csv('avg_tfidf_metrics.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
